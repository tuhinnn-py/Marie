{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Download spacy German and French extensions '''\n",
    "!python -m spacy download de\n",
    "!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using torchtext.datasets.Multi30k Dataset for machine to machine translation\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "from torchtext.data import BucketIterator, Field\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "e = spacy.load('en')\n",
    "g = spacy.load('de')\n",
    "\n",
    "def _tokenize(text : str, src : bool = False) -> List:\n",
    "    _ = e if src else g\n",
    "    return [tok.text for tok in _.tokenizer(text)]\n",
    "\n",
    "'''\n",
    "Preprocessing data through torchtext.data.Field's __init__ pipeline\n",
    "'''\n",
    "\n",
    "e_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, True), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n",
    "g_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, False), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n",
    "\n",
    "train_data, validation_data, test_data = Multi30k.splits(exts = ('.en', '.de'), fields = (e_, g_))\n",
    "\n",
    "e_.build_vocab(train_data, max_size = 100000, min_freq = 0)\n",
    "g_.build_vocab(train_data, max_size = 100000, min_freq = 0)\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = 16, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Using Custom Dataset which has been preprocessed and stored in a csv file \n",
    "\"\"\"\n",
    "\n",
    "''' Reading csv files to buffer '''\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torchtext.data import BucketIterator, Field, TabularDataset\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "x = pd.read_csv(\"../input/-tutils/eng-fr.csv\").drop(columns = \"Unnamed: 0\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "e = spacy.load('en')\n",
    "f = spacy.load('fr')\n",
    "\n",
    "def _tokenize(text : str, src : bool = False) -> List:\n",
    "    _ = e if src else f\n",
    "    return [tok.text for tok in _.tokenizer(text)]\n",
    "\n",
    "'''\n",
    "Preprocessing data through torchtext.data.Field's __init__ pipeline\n",
    "'''\n",
    "\n",
    "e_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, True), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n",
    "f_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, False), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n",
    "\n",
    "fields = {'eng' : ('src', e_), 'fr' : ('trg', f_)}\n",
    "train_data = TabularDataset.splits(path = '../input/-tutils', train = 'eng-fr.csv', format = 'csv', fields = fields)\n",
    "\n",
    "e_.build_vocab(train_data[0], max_size = 100000, min_freq = 0)\n",
    "f_.build_vocab(train_data[0], max_size = 100000, min_freq = 0)\n",
    "\n",
    "train_iterator = BucketIterator.splits(train_data, shuffle = True, batch_size = 32, device = device)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instantiate a Transformer object and predefine the Adam optimizer and training hyperparameters \n",
    "\"\"\"\n",
    "from torch_utils import Transformer, generate_square_subsequent_mask, generate_padding_mask\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 3\n",
    "SRC_VOCAB_SIZE = len(e_.vocab)\n",
    "TGT_VOCAB_SIZE = len(f_.vocab)\n",
    "\n",
    "SRC_PAD_IDX = e_.vocab.stoi['<pad>']\n",
    "TRG_PAD_IDX = f_.vocab.stoi['<pad>']\n",
    "\n",
    "model = Transformer(5000, SRC_PAD_IDX, TRG_PAD_IDX, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, nhead = 16, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 2048, dropout = 0.08).to(device = device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-4, betas = (0.9, 0.98), eps = 1e-08)\n",
    "\n",
    "\"\"\"\n",
    "def generate_padding_mask(src : Tensor, src_field : Field, pad : str = '<pad>', num_heads : int = 8)->Tensor:\n",
    "    '''\n",
    "    Shape : \n",
    "        - src : N, S\n",
    "    where N is the batch size and S is the source sequence length\n",
    "    '''\n",
    "    bsz = src.shape[0]\n",
    "    pad_idx = src_field.vocab.stoi[pad]\n",
    "    out = (src == pad_idx).float()\n",
    "    \n",
    "    out_ = torch.reshape(out, (bsz, -1, 1))\n",
    "    out = torch.reshape(out, (bsz, 1, -1))\n",
    "    out = torch.bmm(out_, out)\n",
    "\n",
    "    out = out.masked_fill(out == 1, float('-inf')).masked_fill(out == 0, float(0.0))  \n",
    "    return torch.cat([out for _ in range(num_heads)], dim = 0)\n",
    "\"\"\"\n",
    "\n",
    "def train_model(model : nn.Module = model, epochs : int = EPOCHS, bkpt_idx : int  = 100) -> None:\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    def schedule_lr(optim, step_num : int, d_model : int = 512, warmup_steps = 4000) -> None:\n",
    "        lr_ =  d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (1.5))\n",
    "        for g in optim.param_groups:\n",
    "                g['lr'] = lr_\n",
    "        return lr_\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for idx, batch in enumerate(train_iterator):\n",
    "            src, tgt = batch.src.long(), batch.trg.long()\n",
    "            '''\n",
    "            Shape : \n",
    "                src - (S, N)\n",
    "                tgt - (T, N)\n",
    "            where S is the source length, T is the target length, N is the number of batches\n",
    "            assert src.shape[1] == tgt.shape[1], Invalid shape Exception\n",
    "            '''\n",
    "            #lr_ = schedule_lr(optim, idx * src.shape[1] + 1)\n",
    "            trg = tgt[:-1, :]\n",
    "            targets = tgt[1:, ].contiguous().view(-1)\n",
    "            \n",
    "            #src_mask = generate_padding_mask(src.transpose(0, 1), e_)\n",
    "            tgt_mask = generate_square_subsequent_mask(trg.shape[0]).to(device)\n",
    "            #tgt_mask = torch.stack([tgt_mask for _ in range(tgt.shape[1] * 8)], dim = 0) + generate_padding_mask(trg.transpose(0, 1), g_)\n",
    "            src_key_padding_mask = generate_padding_mask(src.transpose(0, 1), e_)\n",
    "            tgt_key_padding_mask = generate_padding_mask(trg.transpose(0, 1), f_)\n",
    "            \n",
    "            '''\n",
    "            Shape : \n",
    "                tgt_mask - (T, T)\n",
    "                src_key_padding_mask - (N, S)\n",
    "                tgt_key_padding_mask - (N, T)\n",
    "            where T is the target length which is broadcasted over N, N is the number of batches\n",
    "            '''\n",
    "            \n",
    "            out = model(src, trg, src_key_padding_mask = src_key_padding_mask, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_key_padding_mask)    \n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = F.cross_entropy(out.view(-1, out.shape[-1]), targets, ignore_index = TRG_PAD_IDX)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        if((i + 1) % bkpt_idx == 0):\n",
    "            print(f\"Epoch : {i + 1}   Loss : {loss.item()} LR : {1e-04}\")\n",
    "            #state_dict = {'mod_' : model.state_dict(), 'optim_' : optim.state_dict(), 'loss' : loss.item()}\n",
    "            #torch.save(state_dict, \"./ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the instantiated Transformer class\n",
    "\"\"\"\n",
    "train_model(bkpt_idx = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Visualise some of the training examples \n",
    "'''\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "\" \".join(list(map(lambda x : e_.vocab.itos[x], list(batch.src[:, 0].cpu().numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Visualize inferred translations from the model\n",
    "'''\n",
    "\n",
    "from torch import Tensor\n",
    "from torchtext.data import Field\n",
    "import torch.nn.functional as F\n",
    "g_ = f_   \n",
    "def decode_str(text : Tensor) -> str:\n",
    "    text = text[1:-1, :].transpose(0, 1)[0]\n",
    "    print(\" >> \", \" \".join(list(map(lambda x : g_.vocab.itos[x], list(text.numpy())))), sep = \" \")\n",
    "\n",
    "def get_custom_str(text : str, field : Field, device : str = device) -> Tensor:\n",
    "    text = (lambda x : _tokenize(x, True))(text.lower())\n",
    "    res_ = torch.zeros((len(text) + 2, 1)).to(device)\n",
    "    \n",
    "    out = e_.numericalize([text]).to(device)\n",
    "    res_[1:-1, :] = out\n",
    "    \n",
    "    res_[0] = torch.full((1, 1), fill_value = field.vocab.stoi['<sos>'], dtype = res_.dtype, device = device)\n",
    "    res_[-1] = torch.full((1, 1), fill_value = field.vocab.stoi['<eos>'], dtype = res_.dtype, device = device)\n",
    "    \n",
    "    return res_.long()\n",
    "\n",
    "org_str = \"Lift your hand if you can hear me.\" \n",
    "print(\" >> \", org_str)\n",
    "src = get_custom_str(org_str, e_)\n",
    "trg = torch.full((len(g_.vocab), 1), fill_value = g_.vocab.stoi['<pad>'], dtype = torch.long, device = device)\n",
    "trg[2] = torch.full((1, 1), fill_value = g_.vocab.stoi['<sos>'], dtype = trg.dtype, device = device)\n",
    "\n",
    "for idx in range(len(g_.vocab)):\n",
    "    tgt_mask = generate_square_subsequent_mask(idx + 3).to(device)\n",
    "    src_key_padding_mask = generate_padding_mask(src.transpose(0, 1), e_)\n",
    "    tgt_key_padding_mask = generate_padding_mask(trg[:idx + 3, :].transpose(0, 1), g_)\n",
    "\n",
    "    out = torch.argmax(F.softmax(model(src, trg[:idx + 3, :], src_key_padding_mask = src_key_padding_mask, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_key_padding_mask), dim = -1), dim = -1)\n",
    "    trg[idx + 3, :] = out[-1, :]\n",
    "    \n",
    "    if out[-1, 0] == g_.vocab.stoi['<eos>']:\n",
    "        decode_str(trg[2:idx + 4, :].cpu())\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Loading a pretrained transformer\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "ckpt_ = torch.load(\"../input/transformer/ckpt.pt\")\n",
    "\n",
    "from torch_utils import Transformer, generate_square_subsequent_mask, generate_padding_mask\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SRC_VOCAB_SIZE = len(e_.vocab)\n",
    "TGT_VOCAB_SIZE = len(g_.vocab)\n",
    "\n",
    "SRC_PAD_IDX = e_.vocab.stoi['<pad>']\n",
    "TRG_PAD_IDX = g_.vocab.stoi['<pad>']\n",
    "model = Transformer(SRC_PAD_IDX, TRG_PAD_IDX, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, nhead = 16, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 1024, dropout = 0.0).to(device = device)\n",
    "model.load_state_dict(ckpt_[\"mod_\"], strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Convert text files into Tabular Dataset\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"../input/englishfrenchtranslation/english.txt\", \"r\") as e_open:\n",
    "    with open(\"../input/englishfrenchtranslation/french.txt\", \"r\") as f_open:\n",
    "        ''' Delimiter lambda function '''\n",
    "        func = lambda txt : txt[:-1]\n",
    "        eng, fr = list(map(func, e_open.readlines())), list(map(func, f_open.readlines()))\n",
    "        assert len(eng) == len(fr), \"Lengths of source and target must be same\"\n",
    "        \n",
    "        data = {'eng' : eng, 'fr' : fr}\n",
    "        x = pd.DataFrame(data = data)\n",
    "        \n",
    "        ''' Write the data to a csv file '''\n",
    "        x.to_csv(\"./eng-fr.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
