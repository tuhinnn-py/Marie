{"cells":[{"metadata":{"_uuid":"1aa0c151-5fa4-4370-90d8-6be5d9c14724","_cell_guid":"3171bd22-dfa7-491e-9f95-31956c6999fe","trusted":true},"cell_type":"code","source":"# %% [code]\n''' Download spacy German and French extensions '''\n!python -m spacy download de\n!python -m spacy download fr\n\n# %% [code]\n\"\"\"\nUsing torchtext.datasets.Multi30k Dataset for machine to machine translation\n\"\"\"\n\nimport spacy\nfrom torchtext.data import BucketIterator, Field\nfrom torchtext.datasets import Multi30k\nfrom typing import List\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ne = spacy.load('en')\ng = spacy.load('de')\n\ndef _tokenize(text : str, src : bool = False) -> List:\n    _ = e if src else g\n    return [tok.text for tok in _.tokenizer(text)]\n\n'''\nPreprocessing data through torchtext.data.Field's __init__ pipeline\n'''\n\ne_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, True), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\ng_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, False), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n\ntrain_data, validation_data, test_data = Multi30k.splits(exts = ('.en', '.de'), fields = (e_, g_))\n\ne_.build_vocab(train_data, max_size = 100000, min_freq = 0)\ng_.build_vocab(train_data, max_size = 100000, min_freq = 0)\n\ntrain_iterator, validation_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = 16, device = device)\n\n# %% [code]\n\"\"\" \nUsing Custom Dataset which has been preprocessed and stored in a csv file \n\"\"\"\n\n''' Reading csv files to buffer '''\nimport pandas as pd\nimport spacy\nfrom torchtext.data import BucketIterator, Field, TabularDataset\nfrom typing import List\nimport torch\n\nx = pd.read_csv(\"../input/-tutils/eng-fr.csv\").drop(columns = \"Unnamed: 0\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ne = spacy.load('en')\nf = spacy.load('fr')\n\ndef _tokenize(text : str, src : bool = False) -> List:\n    _ = e if src else f\n    return [tok.text for tok in _.tokenizer(text)]\n\n'''\nPreprocessing data through torchtext.data.Field's __init__ pipeline\n'''\n\ne_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, True), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\nf_ = Field(sequential = True, use_vocab = True, tokenize = lambda x : _tokenize(x, False), lower = True, init_token = '<sos>', eos_token = '<eos>', pad_first = True)\n\nfields = {'eng' : ('src', e_), 'fr' : ('trg', f_)}\ntrain_data = TabularDataset.splits(path = '../input/-tutils', train = 'eng-fr.csv', format = 'csv', fields = fields)\n\ne_.build_vocab(train_data[0], max_size = 100000, min_freq = 0)\nf_.build_vocab(train_data[0], max_size = 100000, min_freq = 0)\n\ntrain_iterator = BucketIterator.splits(train_data, shuffle = True, batch_size = 32, device = device)[0]\n\n# %% [code]\n\"\"\"\nInstantiate a Transformer object and predefine the Adam optimizer and training hyperparameters \n\"\"\"\nfrom torch_utils import Transformer, generate_square_subsequent_mask, generate_padding_mask\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nEPOCHS = 3\nSRC_VOCAB_SIZE = len(e_.vocab)\nTGT_VOCAB_SIZE = len(f_.vocab)\n\nSRC_PAD_IDX = e_.vocab.stoi['<pad>']\nTRG_PAD_IDX = f_.vocab.stoi['<pad>']\n\nmodel = Transformer(5000, SRC_PAD_IDX, TRG_PAD_IDX, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, nhead = 16, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 2048, dropout = 0.08).to(device = device)\noptim = torch.optim.Adam(model.parameters(), lr = 1e-4, betas = (0.9, 0.98), eps = 1e-08)\n\n\"\"\"\ndef generate_padding_mask(src : Tensor, src_field : Field, pad : str = '<pad>', num_heads : int = 8)->Tensor:\n    '''\n    Shape : \n        - src : N, S\n    where N is the batch size and S is the source sequence length\n    '''\n    bsz = src.shape[0]\n    pad_idx = src_field.vocab.stoi[pad]\n    out = (src == pad_idx).float()\n    \n    out_ = torch.reshape(out, (bsz, -1, 1))\n    out = torch.reshape(out, (bsz, 1, -1))\n    out = torch.bmm(out_, out)\n\n    out = out.masked_fill(out == 1, float('-inf')).masked_fill(out == 0, float(0.0))  \n    return torch.cat([out for _ in range(num_heads)], dim = 0)\n\"\"\"\n\ndef train_model(model : nn.Module = model, epochs : int = EPOCHS, bkpt_idx : int  = 100) -> None:\n    model.train()\n    \n    \n    def schedule_lr(optim, step_num : int, d_model : int = 512, warmup_steps = 4000) -> None:\n        lr_ =  d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (1.5))\n        for g in optim.param_groups:\n                g['lr'] = lr_\n        return lr_\n    \n    for i in range(epochs):\n        for idx, batch in enumerate(train_iterator):\n            src, tgt = batch.src.long(), batch.trg.long()\n            '''\n            Shape : \n                src - (S, N)\n                tgt - (T, N)\n            where S is the source length, T is the target length, N is the number of batches\n            assert src.shape[1] == tgt.shape[1], Invalid shape Exception\n            '''\n            #lr_ = schedule_lr(optim, idx * src.shape[1] + 1)\n            trg = tgt[:-1, :]\n            targets = tgt[1:, ].contiguous().view(-1)\n            \n            #src_mask = generate_padding_mask(src.transpose(0, 1), e_)\n            tgt_mask = generate_square_subsequent_mask(trg.shape[0]).to(device)\n            #tgt_mask = torch.stack([tgt_mask for _ in range(tgt.shape[1] * 8)], dim = 0) + generate_padding_mask(trg.transpose(0, 1), g_)\n            src_key_padding_mask = generate_padding_mask(src.transpose(0, 1), e_)\n            tgt_key_padding_mask = generate_padding_mask(trg.transpose(0, 1), f_)\n            \n            '''\n            Shape : \n                tgt_mask - (T, T)\n                src_key_padding_mask - (N, S)\n                tgt_key_padding_mask - (N, T)\n            where T is the target length which is broadcasted over N, N is the number of batches\n            '''\n            \n            out = model(src, trg, src_key_padding_mask = src_key_padding_mask, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_key_padding_mask)    \n            optim.zero_grad()\n\n            loss = F.cross_entropy(out.view(-1, out.shape[-1]), targets, ignore_index = TRG_PAD_IDX)\n            loss.backward()\n            optim.step()\n            \n        if((i + 1) % bkpt_idx == 0):\n            print(f\"Epoch : {i + 1}   Loss : {loss.item()} LR : {1e-04}\")\n            #state_dict = {'mod_' : model.state_dict(), 'optim_' : optim.state_dict(), 'loss' : loss.item()}\n            #torch.save(state_dict, \"./ckpt.pt\")\n\n# %% [code]\n\"\"\"\nTrain the instantiated Transformer class\n\"\"\"\ntrain_model(bkpt_idx = 1)\n\n# %% [code]\n''' \nVisualise some of the training examples \n'''\n\nbatch = next(iter(train_iterator))\n\" \".join(list(map(lambda x : e_.vocab.itos[x], list(batch.src[:, 0].cpu().numpy()))))\n\n# %% [code]\n''' \nVisualize inferred translations from the model\n'''\n\nfrom torch import Tensor\nfrom torchtext.data import Field\nimport torch.nn.functional as F\ng_ = f_   \ndef decode_str(text : Tensor) -> str:\n    text = text[1:-1, :].transpose(0, 1)[0]\n    print(\" >> \", \" \".join(list(map(lambda x : g_.vocab.itos[x], list(text.numpy())))), sep = \" \")\n\ndef get_custom_str(text : str, field : Field, device : str = device) -> Tensor:\n    text = (lambda x : _tokenize(x, True))(text.lower())\n    res_ = torch.zeros((len(text) + 2, 1)).to(device)\n    \n    out = e_.numericalize([text]).to(device)\n    res_[1:-1, :] = out\n    \n    res_[0] = torch.full((1, 1), fill_value = field.vocab.stoi['<sos>'], dtype = res_.dtype, device = device)\n    res_[-1] = torch.full((1, 1), fill_value = field.vocab.stoi['<eos>'], dtype = res_.dtype, device = device)\n    \n    return res_.long()\n\norg_str = \"Lift your hand if you can hear me.\" \nprint(\" >> \", org_str)\nsrc = get_custom_str(org_str, e_)\ntrg = torch.full((len(g_.vocab), 1), fill_value = g_.vocab.stoi['<pad>'], dtype = torch.long, device = device)\ntrg[2] = torch.full((1, 1), fill_value = g_.vocab.stoi['<sos>'], dtype = trg.dtype, device = device)\n\nfor idx in range(len(g_.vocab)):\n    tgt_mask = generate_square_subsequent_mask(idx + 3).to(device)\n    src_key_padding_mask = generate_padding_mask(src.transpose(0, 1), e_)\n    tgt_key_padding_mask = generate_padding_mask(trg[:idx + 3, :].transpose(0, 1), g_)\n\n    out = torch.argmax(F.softmax(model(src, trg[:idx + 3, :], src_key_padding_mask = src_key_padding_mask, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_key_padding_mask), dim = -1), dim = -1)\n    trg[idx + 3, :] = out[-1, :]\n    \n    if out[-1, 0] == g_.vocab.stoi['<eos>']:\n        decode_str(trg[2:idx + 4, :].cpu())\n        break    \n\n# %% [code]\n\"\"\" \nLoading a pretrained transformer\n\"\"\"\n\nimport torch\nckpt_ = torch.load(\"../input/transformer/ckpt.pt\")\n\nfrom torch_utils import Transformer, generate_square_subsequent_mask, generate_padding_mask\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nSRC_VOCAB_SIZE = len(e_.vocab)\nTGT_VOCAB_SIZE = len(g_.vocab)\n\nSRC_PAD_IDX = e_.vocab.stoi['<pad>']\nTRG_PAD_IDX = g_.vocab.stoi['<pad>']\nmodel = Transformer(SRC_PAD_IDX, TRG_PAD_IDX, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, nhead = 16, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 1024, dropout = 0.0).to(device = device)\nmodel.load_state_dict(ckpt_[\"mod_\"], strict = True)\n\n# %% [code]\n''' \nConvert text files into Tabular Dataset\n'''\n\nimport pandas as pd\n\nwith open(\"../input/englishfrenchtranslation/english.txt\", \"r\") as e_open:\n    with open(\"../input/englishfrenchtranslation/french.txt\", \"r\") as f_open:\n        ''' Delimiter lambda function '''\n        func = lambda txt : txt[:-1]\n        eng, fr = list(map(func, e_open.readlines())), list(map(func, f_open.readlines()))\n        assert len(eng) == len(fr), \"Lengths of source and target must be same\"\n        \n        data = {'eng' : eng, 'fr' : fr}\n        x = pd.DataFrame(data = data)\n        \n        ''' Write the data to a csv file '''\n        x.to_csv(\"./eng-fr.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}